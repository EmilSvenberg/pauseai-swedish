---
title: Risker med artificiell intelligens
description: AI hotar vår demokrati, vår teknologi och vår art.
---

AI är en kraftfull teknik som i allt större utsträckning förändrar vår värld.
Den kommer med fantastisk potential, men också med allvarliga risker, inklusive [existentiell katastrof](/xrisk).

## Nuvarande faror

### Falska nyheter, polarisering och hot mot demokratin

Mycket av vårt samhälle är baserat på tillit. Vi litar på att pengarna på vårt bankkonto är verkliga, att nyheterna vi läser är sanna, och att personerna som postar recensioner online existerar.

AI-system är exceptionellt bra på att skapa falska medier.
De kan skapa falska videor, falska ljud, falska texter och falska bilder.
Dessa kapaciteter förbättras snabbt.
För bara två år sedan skrattade vi åt de hemskt orealistiska Dall-E-bilderna, men nu har vi [deepfake-bilder som vinner fototävlingar](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Ett 10-sekunders ljudklipp eller en enda bild kan vara nog för att skapa en övertygande deepfake.

Att skapa falska medier är inte nytt, men AI gör det mycket billigare och mycket mer realistiskt.
En AI-genererad bild av en explosion orsakade [panikförsäljning på Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
GPT-4 kan skriva på ett sätt som är omöjligt att skilja från människor, men i en mycket snabbare takt och till en bråkdel av kostnaden.
Vi kanske snart ser sociala medier översvämmas med falska diskussioner och åsikter, och falska nyhetsartiklar som är omöjliga att skilja från riktiga.

Detta leder till polarisering mellan olika grupper av människor som tror på olika informationskällor och narrativ och, genom att konsumera förvrängda representationer av vad som händer, eskalerar sina skillnader tills de kulminerar i våldsamma och antidemokratiska svar.

Ett stopp för frontlinjemodeller (vårt [förslag](/proposal)) skulle inte stoppa de modeller som används idag för att skapa falska medier, men det kan hjälpa till att förhindra framtida avancerade modeller.
Dessutom skulle det lägga grunden för framtida reglering som syftar till att mildra falska medier och andra specifika problem orsakade av AI. För att inte tala om att öka allmänhetens uppmärksamhet och medvetenhet om dessa faror och bevis på att de kan åtgärdas.

### Deepfakes och imitation

Falskt innehåll skapat med AI, även kallat deepfakes, kan inte bara stjäla kända personers identiteter och [skapa desinformation](https://time.com/6565446/biden-deepfake-audio/), utan de kan också imitera dig.
Alla med foton, videor eller ljud av någon och tillräcklig kunskap kan skapa deepfakes av dem och använda dem för att begå bedrägerier, trakassera dem eller skapa sexuellt icke-konsensuellt material.
Ungefär 96% av allt deepfake-innehåll är sexuellt material.

Som avsnittet om falska nyheter säger, skulle falska medier inte helt förhindras av vårt förslag, men de kunde minskas till viss del.
En inte så liten del när man tar i beaktande att AI-multifunktionssystem som chatbots har blivit väldigt populära, och vi skulle stoppa dem från att bli mer kapabla och populära, vilket kan inkludera system designade med färre filter och träningsbara med nya ansikten.

### Fördomar och diskriminering

AI-system tränas på data, och mycket av den data vi har är på något sätt partisk.
Detta innebär att AI-system kommer att ärva samhällets fördomar.
Ett automatiserat rekryteringssystem på Amazon [ärvde en fördom mot kvinnor](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Svarta patienter var [mindre benägna att bli hänvisade till en medicinsk specialist](https://www.science.org/doi/full/10.1126/science.aax2342).
Partiska system som används inom brottsbekämpning, såsom prediktiva polissystem, kan leda till orättvis måltavla mot specifika grupper.
Generativa AI-modeller kopierar inte bara fördomarna från deras träningsdata, [de förstärker dem](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Dessa fördomar uppstår ofta utan att skaparna av AI-systemet är medvetna om dem.

<!-- ### Dataskydd
-->

### Jobbförlust, ekonomisk ojämlikhet och instabilitet

Under den industriella revolutionen förlorade många människor sina jobb till maskiner.
Men nya (ofta bättre) jobb skapades, och ekonomin växte.
Den här gången kan saker och ting vara annorlunda.

AI ersätter inte bara våra muskler som ångmaskinen gjorde, den ersätter våra hjärnor.
Vanliga människor kanske inte har något kvar att erbjuda ekonomin.
Bildgenereringsmodeller (som är kraftigt tränade på upphovsrättsskyddat material från professionella konstnärer) påverkar redan [den kreativa industrin](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Skribenter [strejkar](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 har [klarat advokatexamen](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kan skriva utmärkt innehåll och kan skriva kod (återigen, delvis tränad på [upphovsrättsskyddat material](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

De som äger dessa AI-system kommer att kunna kapitalisera på dem, men de som förlorar sina jobb till dem kommer inte att göra det.
Det är svårt att förutsäga vilka jobb som kommer att ersättas först. De kan lämna dig arbetslös och utan inkomst oavsett hur mycket tid, pengar och energi du spenderat på att få den erfarenhet och kunskap som du har, och hur värdefulla de var för en stund sedan.
Det sätt vi fördelar rikedom i vårt samhälle är inte förberett för detta.

### Mental hälsa, beroende och avskildhet mellan människor

Sociala medier, videospel och annan programvara har redan använt AI-system för att maximera sin vinst medan de utnyttjar våra primathjärnor, vilket skadar vår mentala hälsa i processen.
Beroende av sociala medier, bland annat, isolerar oss från varandra, inte bara i politiska bubblor utan också i kulturella och sociala enmansbubblor, vilket gör oss ensammare.
De är det första beviset på de oavsiktliga och oväntade globala konsekvenser som dessa teknologier kan medföra och hur komplicerat det kan vara att anpassa AI-system med "mänskliga värderingar".

Om dagens chatbots fortsätter att bli bättre, kan det bli ganska vanligt att bli beroende av dem och ersätta hela relationer (vare sig romantiska, sexuella eller platoniska) med dem.
Även om dessa appar är lätta att komma åt, kan de forma förståelsen, personligheten och världsbilden hos barn som kan föredra att prata med AI istället för familj och vänner.
En paus i de största modellerna kan förhindra dem från att bli multifunktionella chatbots som passar våra behov perfekt utan att människor förstår de långsiktiga konsekvenserna av dem.

### Maktkoncentration, krig och kapplöpning mot avgrunden

Beroendet av produkter och tjänster som lär sig av personliga data lämnar oss som maktlösa separerade individer vare sig det är avsiktligt eller inte.
Och det spelar på en ond cirkel med koncentrationen av ekonomisk makt och intelligens hos de företag som skapar dem.

Om denna ekonomiska och teknologiska ojämlikhet härstammar från en handfull offentliga och privata enheter som producerar flera enkeländamåls-AI eller några multifunktions-AI, kan det leda till en kort maktkoncentration som sannolikt resulterar i en katastrof för alla.
Maktsamlingen av den berättelsen har och kommer att fortsätta att incitamentera fler aktörer

att gå med i kapplöpningen till botten och påskynda utvecklingen av större AI-system.
Detta introducerar fler felpunkter och förringar de tillhörande riskerna genom att stödja idén att de kan hanteras unilateralt, av ett företag eller en regering.

Ett sådant scenario skulle inte bara avmaktiga varje annan person och nation i världen, utan också katalysera globala makter att gå i konflikt.
Så det är avgörande att agera så snart som möjligt, innan tävlingsdynamiken sträcker sig längre, innan de redan mäktigaste regeringarna och företagen konsoliderar sina positioner och innan ett krig utlöses som svar.
Vi behöver internationellt samarbete eftersom det enda vinnande draget i detta konstiga spel är att inte spela, utan att pausa.

### Auktoritära regeringar
Auktoritära och totalitära regeringar kan också använda AI-teknologier för att utöva makt över sina territorier och befolkningar.
De kan kontrollera kommunikationskanaler eller upprätthålla sociala kredit- och massövervakningssystem som säkerställer att de behåller sin makt samtidigt som de kränker mänskliga rättigheter.

### Autonoma vapen
Autonoma vapen
Företag säljer redan AI-drivna vapen till regeringar.
Lanius bygger flygande självmordsdrönare som autonomt identifierar fiender.
Palantirs AIP-system använder stora språkmodeller för att analysera slagfältsdata och komma med optimala strategier.

Nationer och vapenföretag har insett att AI kommer att ha en enorm inverkan på att överträffa sina fiender.
Vi har gått in i en ny kapprustning.
Denna dynamik belönar att skynda på och skära hörn.

Just nu har vi fortfarande människor i loopen för dessa vapen.
Men när kapaciteten hos dessa AI-system förbättras kommer det att bli mer och mer tryck att ge maskinerna makten att fatta beslut.
När vi delegerar kontrollen över vapen till AI, kan fel och buggar få fruktansvärda konsekvenser.
Den hastighet med vilken AI kan bearbeta information och fatta beslut kan orsaka att konflikter eskalerar på några minuter.
En nyligen publicerad artikel konstaterar att "modeller tenderar att utveckla kapprustningsdynamik, vilket leder till större konflikter och i sällsynta fall till och med till användning av kärnvapen".

Läs mer på stopkillerrobots.org

## Nära framtida faror
### Biologiska vapen
AI kan göra kunskap mer tillgänglig, vilket också inkluderar kunskap om hur man skapar biologiska vapen. Denna artikel visar hur GPT-4 kan hjälpa icke-vetenskapliga studenter att skapa en pandemi-patogen:

På en timme föreslog chatbotarna fyra potentiella pandemi-patogener, förklarade hur de kan genereras från syntetiskt DNA med hjälp av omvänd genetik, försåg namnen på DNA-syntesföretag som troligen inte screenar beställningar, identifierade detaljerade protokoll och hur man felsöker dem, och rekommenderade att någon som saknar färdigheterna att utföra omvänd genetik engagerar en kärnanläggning eller ett kontraktsforskningsföretag.

Denna typ av kunskap har aldrig varit så tillgänglig, och vi har inte säkerhetsåtgärderna på plats för att hantera de potentiella konsekvenserna.

Dessutom kan vissa AI-modeller användas för att designa helt nya farliga patogener.
En modell kallad MegaSyn designade 40 000 nya kemiska vapen/toxiska molekyler på en timme.
Den revolutionära AlphaFold-modellen kan förutsäga strukturen av proteiner, vilket också är en dual-use technology.
Att förutsäga proteinstrukturer kan användas för att "upptäcka sjukdomsframkallande mutationer med hjälp av en individs genomsekvens".
Forskare skapar nu till och med helt autonoma kemiska laboratorier, där AI-system själva kan syntetisera nya kemikalier.

Den grundläggande faran är att kostnaden för att designa och använda biologiska vapen sänks med magnituder på grund av AI.

### Datorvirus och Cybersäkerhet 
Praktiskt taget allt vi gör idag är på något sätt beroende av datorer.
Vi betalar för våra dagligvaror, planerar våra dagar, kontaktar våra nära och kära och till och med kör våra bilar med datorer.

Moderna AI-system kan analysera och skriva programvara.
De kan hitta sårbarheter i programvara, och de kan användas för att utnyttja dem.
När AI-kapaciteten växer kommer också kapaciteten för de utnyttjanden de kan skapa att växa.

Mycket potenta datorvirus har alltid varit extremt svåra att skapa, men AI kan förändra detta.
Istället för att behöva anlita ett team av skickliga säkerhetsexperter/hackare för att hitta zero-day-sårbarheter, kan du bara använda en mycket billigare AI för att göra det åt dig. Naturligtvis kan AI också hjälpa till med cyberförsvar, och det är oklart på vilken sida fördelen ligger.

Läs mer om AI och cybersäkerhetsrisker

### Existentiell risk
Många AI-forskare varnar för att AI kan leda till mänsklighetens slut.

Mycket intelligenta saker är mycket kraftfulla.
Om vi bygger en maskin som är mycket mer intelligent än människor måste vi vara säkra på att den vill samma sak som vi vill.
Detta visar sig dock vara mycket svårt.
Detta kallas anpassningsproblemet.
Om vi misslyckas med att lösa det i tid kan vi hamna med superintelligenta maskiner som inte bryr sig om vårt välbefinnande.
Vi skulle introducera en ny art
### Existential Risk

Many AI researchers are warning that AI could lead to the end of humanity.

Very intelligent things are very powerful.
If we build a machine that is far more intelligent than humans, we need to be sure that it wants the same thing as we want.
However, this turns out to be very difficult.
This is called the _alignment problem_.
If we fail to solve it in time, we may end up with superintelligent machines that do not care about our well-being.
We'd be introducing a new species to the planet that could outsmart us and outcompete us.

[Read more about x-risk](/xrisk)

### Human disempowerment

Even if we manage to create only AI systems that we can control individually, we could lose our power to make important decisions incrementally each time one becomes implemented inside institutions or popularized in everyday life.
Those systems would end up having more input from other systems than from humans, and, if we cannot coordinate quickly enough, or we lack crucial knowledge about the functioning of these systems, we could end up without control over our future.

It would be a civilization in which each system is optimizing for different objectives, there is not a clear direction for where everything is heading, and there is no way of changing it.
The technical knowledge required to modify these systems could be lacking in the first place or lost over time, as we become more and more dependent on technology, and the technology becomes more complex.

The systems may achieve their goals, but those goals might not entirely encapsulate the values they were expected to. This problem is, to a certain extent, already happening today, but AIs could significantly amplify it.

### Digital sentience

As AI continues to advance, future systems may become incredibly sophisticated, replicating neural structures and functions that are more akin to the human brain.
This increased complexity might lead to emergent properties like subjectivity and/or consciousness, so those AIs would be deserving of moral considerations and be treated well.
Would be like "digital people".
The thing is that, given our present lack of knowledge about consciousness and the nature of neural networks, we won't have a way to determine whether some AIs would have any type of experience and what the quality of those experiences would depend on.
If the AIs continue to be produced with only their capabilities in mind, through a process we don't fully understand, people will keep on using them as tools ignoring what their desires could be, and that they could be actually enslaving digital people.

### Suffering lock-in risks

It is possible that once automation at higher degrees starts happening, regardless if there is just one or multiple powerful AIs, the values of those systems would not be able to be changed, and the automation would continue until the end of the universe, throughout the reachable galaxies.
Arguably, the worst scenarios that those AIs could create would not be human extinction, but inescapable dystopias that would extend through all that spacetime.

Possible locked-in dystopias with lots of suffering are called _S-risks_ and include worlds in which sentient beings are enslaved and forced to do horrible things.
Those beings could be humans, animals, digital people or any other alien species that the system could find in the cosmos. Given how difficult we think solving alignment completely is, how bad we humans treat each other sometimes, how bad we treat most animals, and how we treat present AIs, a future like that is maybe not as unlikely as we'd like.

## What can we do?

For **all** the problems discussed above, the risk increases as AI capabilities improve.
This means that the safest thing to do now is to **slow down**.
We need to pause the development of more powerful AI systems until we have figured out how to deal with the risks.

See [our proposal](/proposal) for more details.
