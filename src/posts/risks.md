---
title: Risker med artificiell intelligens
description: AI hotar vår demokrati, vår teknologi och vår art.
---

AI är en kraftfull teknik som i allt större utsträckning förändrar vår värld.
Den kommer med fantastisk potential, men också med allvarliga risker, inklusive [existentiell katastrof](/xrisk).

## Nuvarande faror

### Falska nyheter, polarisering och hot mot demokratin

Mycket av vårt samhälle är baserat på tillit. Vi litar på att pengarna på vårt bankkonto är verkliga, att nyheterna vi läser är sanna, och att personerna som postar recensioner online existerar.

AI-system är exceptionellt bra på att skapa falska medier.
De kan skapa falska videor, falska ljud, falska texter och falska bilder.
Dessa kapaciteter förbättras snabbt.
För bara två år sedan skrattade vi åt de hemskt orealistiska Dall-E-bilderna, men nu har vi [deepfake-bilder som vinner fototävlingar](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Ett 10-sekunders ljudklipp eller en enda bild kan vara nog för att skapa en övertygande deepfake.

Att skapa falska medier är inte nytt, men AI gör det mycket billigare och mycket mer realistiskt.
En AI-genererad bild av en explosion orsakade [panikförsäljning på Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
GPT-4 kan skriva på ett sätt som är omöjligt att skilja från människor, men i en mycket snabbare takt och till en bråkdel av kostnaden.
Vi kanske snart ser sociala medier översvämmas med falska diskussioner och åsikter, och falska nyhetsartiklar som är omöjliga att skilja från riktiga.

Detta leder till polarisering mellan olika grupper av människor som tror på olika informationskällor och narrativ och, genom att konsumera förvrängda representationer av vad som händer, eskalerar sina skillnader tills de kulminerar i våldsamma och antidemokratiska svar.

Ett stopp för frontlinjemodeller (vårt [förslag](/proposal)) skulle inte stoppa de modeller som används idag för att skapa falska medier, men det kan hjälpa till att förhindra framtida avancerade modeller.
Dessutom skulle det lägga grunden för framtida reglering som syftar till att mildra falska medier och andra specifika problem orsakade av AI. För att inte tala om att öka allmänhetens uppmärksamhet och medvetenhet om dessa faror och bevis på att de kan åtgärdas.

### Deepfakes och imitation

Falskt innehåll skapat med AI, även kallat deepfakes, kan inte bara stjäla kända personers identiteter och [skapa desinformation](https://time.com/6565446/biden-deepfake-audio/), utan de kan också imitera dig.
Alla med foton, videor eller ljud av någon och tillräcklig kunskap kan skapa deepfakes av dem och använda dem för att begå bedrägerier, trakassera dem eller skapa sexuellt icke-konsensuellt material.
Ungefär 96% av allt deepfake-innehåll är sexuellt material.

Som avsnittet om falska nyheter säger, skulle falska medier inte helt förhindras av vårt förslag, men de kunde minskas till viss del.
En inte så liten del när man tar i beaktande att AI-multifunktionssystem som chatbots har blivit väldigt populära, och vi skulle stoppa dem från att bli mer kapabla och populära, vilket kan inkludera system designade med färre filter och träningsbara med nya ansikten.

### Fördomar och diskriminering

AI-system tränas på data, och mycket av den data vi har är på något sätt partisk.
Detta innebär att AI-system kommer att ärva samhällets fördomar.
Ett automatiserat rekryteringssystem på Amazon [ärvde en fördom mot kvinnor](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Svarta patienter var [mindre benägna att bli hänvisade till en medicinsk specialist](https://www.science.org/doi/full/10.1126/science.aax2342).
Partiska system som används inom brottsbekämpning, såsom prediktiva polissystem, kan leda till orättvis måltavla mot specifika grupper.
Generativa AI-modeller kopierar inte bara fördomarna från deras träningsdata, [de förstärker dem](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Dessa fördomar uppstår ofta utan att skaparna av AI-systemet är medvetna om dem.

<!-- ### Dataskydd
-->

### Jobbförlust, ekonomisk ojämlikhet och instabilitet

Under den industriella revolutionen förlorade många människor sina jobb till maskiner.
Men nya (ofta bättre) jobb skapades, och ekonomin växte.
Den här gången kan saker och ting vara annorlunda.

AI ersätter inte bara våra muskler som ångmaskinen gjorde, den ersätter våra hjärnor.
Vanliga människor kanske inte har något kvar att erbjuda ekonomin.
Bildgenereringsmodeller (som är kraftigt tränade på upphovsrättsskyddat material från professionella konstnärer) påverkar redan [den kreativa industrin](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Skribenter [strejkar](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 har [klarat advokatexamen](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kan skriva utmärkt innehåll och kan skriva kod (återigen, delvis tränad på [upphovsrättsskyddat material](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

De som äger dessa AI-system kommer att kunna kapitalisera på dem, men de som förlorar sina jobb till dem kommer inte att göra det.
Det är svårt att förutsäga vilka jobb som kommer att ersättas först. De kan lämna dig arbetslös och utan inkomst oavsett hur mycket tid, pengar och energi du spenderat på att få den erfarenhet och kunskap som du har, och hur värdefulla de var för en stund sedan.
Det sätt vi fördelar rikedom i vårt samhälle är inte förberett för detta.

### Mental hälsa, beroende och avskildhet mellan människor

Sociala medier, videospel och annan programvara har redan använt AI-system för att maximera sin vinst medan de utnyttjar våra primathjärnor, vilket skadar vår mentala hälsa i processen.
Beroende av sociala medier, bland annat, isolerar oss från varandra, inte bara i politiska bubblor utan också i kulturella och sociala enmansbubblor, vilket gör oss ensammare.
De är det första beviset på de oavsiktliga och oväntade globala konsekvenser som dessa teknologier kan medföra och hur komplicerat det kan vara att anpassa AI-system med "mänskliga värderingar".

Om dagens chatbots fortsätter att bli bättre, kan det bli ganska vanligt att bli beroende av dem och ersätta hela relationer (vare sig romantiska, sexuella eller platoniska) med dem.
Även om dessa appar är lätta att komma åt, kan de forma förståelsen, personligheten och världsbilden hos barn som kan föredra att prata med AI istället för familj och vänner.
En paus i de största modellerna kan förhindra dem från att bli multifunktionella chatbots som passar våra behov perfekt utan att människor förstår de långsiktiga konsekvenserna av dem.

### Maktkoncentration, krig och kapplöpning mot avgrunden

Beroendet av produkter och tjänster som lär sig av personliga data lämnar oss som maktlösa separerade individer vare sig det är avsiktligt eller inte.
Och det spelar på en ond cirkel med koncentrationen av ekonomisk makt och intelligens hos de företag som skapar dem.

Om denna ekonomiska och teknologiska ojämlikhet härstammar från en handfull offentliga och privata enheter som producerar flera enkeländamåls-AI eller några multifunktions-AI, kan det leda till en kort maktkoncentration som sannolikt resulterar i en katastrof för alla.
Maktsamlingen av den berättelsen har och kommer att fortsätta att incitamentera fler aktörer

att gå med i kapplöpningen till botten och påskynda utvecklingen av större AI-system.
Detta introducerar fler felpunkter och förringar de tillhörande riskerna genom att stödja idén att de kan hanteras unilateralt, av ett företag eller en regering.

Ett sådant scenario skulle inte bara avmaktiga varje annan person och nation i världen, utan också katalysera globala makter att gå i konflikt.
Så det är avgörande att agera så snart som möjligt, innan tävlingsdynamiken sträcker sig längre, innan de redan mäktigaste regeringarna och företagen konsoliderar sina positioner och innan ett krig utlöses som svar.
Vi behöver internationellt samarbete eftersom det enda vinnande draget i detta konstiga spel är att inte spela, utan att pausa.

### Auktoritära regeringar
Auktoritära och totalitära regeringar kan också använda AI-teknologier för att utöva makt över sina territorier och befolkningar.
De kan kontrollera kommunikationskanaler eller upprätthålla sociala kredit- och massövervakningssystem som säkerställer att de behåller sin makt samtidigt som de kränker mänskliga rättigheter.

### Autonoma vapen
Företag säljer redan AI-drivna vapen till regeringar.
Lanius bygger flygande självmordsdrönare som autonomt identifierar fiender.
Palantirs AIP-system använder stora språkmodeller för att analysera stridsfältdata och komma med optimala strategier.

Nationer och vapenföretag har insett att AI kommer att ha en enorm inverkan på


### Autonomous weapons

Companies are already selling AI-powered weapons to governments.
Lanius builds [flying suicide drones](https://www.youtube.com/watch?v=G7yIzY1BxuI) that autonomously identify foes.
Palantir's [AIP system](https://www.youtube.com/watch?v=XEM5qz__HOU) uses large language models to analyze battlefield data and come up with optimal strategies.

Nations and weapon companies have realized that AI will have a huge impact on besting their enemies.
We've entered a new arms race.
This dynamic rewards speeding up and cutting corners.

Right now, we still have humans in the loop for these weapons.
But as the capabilities of these AI systems improve, there will be more and more pressure to give the machines the power to decide.
When we delegate control of weapons to AI, errors and bugs could have horrible consequences.
The speed at which AI can process information and make decisions may cause conflicts to escalate in minutes.
A [recent paper](https://arxiv.org/pdf/2401.03408.pdf) concludes that "models tend to develop arms-race dynamics, leading to greater conflict, and in rare cases, even to the deployment of nuclear weapons".

Read more at [stopkillerrobots.org](https://www.stopkillerrobots.org/military-and-killer-robots/)

## Near future dangers

### Biological weapons

AI can make knowledge more accessible, which also includes knowledge about how to create biological weapons. [This paper](https://arxiv.org/abs/2306.03809) shows how GPT-4 can help non-scientist students to create a pandemic pathogen:

> In one hour, the chatbots suggested four potential pandemic pathogens, explained how they can be generated from synthetic DNA using reverse genetics, supplied the names of DNA synthesis companies unlikely to screen orders, identified detailed protocols and how to troubleshoot them, and recommended that anyone lacking the skills to perform reverse genetics engage a core facility or contract research organization.

This type of knowledge has never been so accessible, and we do not have the safeguards in place to deal with the potential consequences.

Additionally, some AI models can be used to design completely new hazardous pathogens.
A model called MegaSyn designed [40,000 new chemical weapons / toxic molecules in one hour](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
The revolutionary AlphaFold model can predict the structure of proteins, which is also a [dual-use technology](https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf).
Predicting protein structures can be used to "discover disease-causing mutations using one individual’s genome sequence".
Scientists are now even creating [fully autonomous chemical labs, where AI systems can synthesize new chemicals on their own](https://twitter.com/andrewwhite01/status/1670794000398184451).

The fundamental danger is that the cost of designing and applying biological weapons is being lowered by orders of magnitude because of AI.

### Computer viruses and hacks

Virtually everything we do nowadays is in some way dependent on computers.
We pay for our groceries, plan our days, contact our loved ones and even drive our cars with computers.

Modern AI systems can analyze and write software.
They [can find vulnerabilities](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411) in software, and [they could be used to exploit them](https://blog.checkpoint.com/2023/03/15/check-point-research-conducts-initial-security-analysis-of-chatgpt4-highlighting-potential-scenarios-for-accelerated-cybercrime/).
As AI capabilities grow, so will the capabilities of the exploits they can create.

Highly potent computer viruses have always been extremely hard to create, but AI could change that.
Instead of having to hire a team of skilled security experts/hackers to find zero-day exploits, you could just use a far cheaper AI to do it for you. Of course, AI could also help with cyberdefense, and it is unclear on which side the advantage lies.

[Read more about AI and cybersecurity risks](/cybersecurity-risks)

### Existential Risk

Many AI researchers are warning that AI could lead to the end of humanity.

Very intelligent things are very powerful.
If we build a machine that is far more intelligent than humans, we need to be sure that it wants the same thing as we want.
However, this turns out to be very difficult.
This is called the _alignment problem_.
If we fail to solve it in time, we may end up with superintelligent machines that do not care about our well-being.
We'd be introducing a new species to the planet that could outsmart us and outcompete us.

[Read more about x-risk](/xrisk)

### Human disempowerment

Even if we manage to create only AI systems that we can control individually, we could lose our power to make important decisions incrementally each time one becomes implemented inside institutions or popularized in everyday life.
Those systems would end up having more input from other systems than from humans, and, if we cannot coordinate quickly enough, or we lack crucial knowledge about the functioning of these systems, we could end up without control over our future.

It would be a civilization in which each system is optimizing for different objectives, there is not a clear direction for where everything is heading, and there is no way of changing it.
The technical knowledge required to modify these systems could be lacking in the first place or lost over time, as we become more and more dependent on technology, and the technology becomes more complex.

The systems may achieve their goals, but those goals might not entirely encapsulate the values they were expected to. This problem is, to a certain extent, already happening today, but AIs could significantly amplify it.

### Digital sentience

As AI continues to advance, future systems may become incredibly sophisticated, replicating neural structures and functions that are more akin to the human brain.
This increased complexity might lead to emergent properties like subjectivity and/or consciousness, so those AIs would be deserving of moral considerations and be treated well.
Would be like "digital people".
The thing is that, given our present lack of knowledge about consciousness and the nature of neural networks, we won't have a way to determine whether some AIs would have any type of experience and what the quality of those experiences would depend on.
If the AIs continue to be produced with only their capabilities in mind, through a process we don't fully understand, people will keep on using them as tools ignoring what their desires could be, and that they could be actually enslaving digital people.

### Suffering lock-in risks

It is possible that once automation at higher degrees starts happening, regardless if there is just one or multiple powerful AIs, the values of those systems would not be able to be changed, and the automation would continue until the end of the universe, throughout the reachable galaxies.
Arguably, the worst scenarios that those AIs could create would not be human extinction, but inescapable dystopias that would extend through all that spacetime.

Possible locked-in dystopias with lots of suffering are called _S-risks_ and include worlds in which sentient beings are enslaved and forced to do horrible things.
Those beings could be humans, animals, digital people or any other alien species that the system could find in the cosmos. Given how difficult we think solving alignment completely is, how bad we humans treat each other sometimes, how bad we treat most animals, and how we treat present AIs, a future like that is maybe not as unlikely as we'd like.

## What can we do?

For **all** the problems discussed above, the risk increases as AI capabilities improve.
This means that the safest thing to do now is to **slow down**.
We need to pause the development of more powerful AI systems until we have figured out how to deal with the risks.

See [our proposal](/proposal) for more details.
