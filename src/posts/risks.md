---
title: Risker med artificiell intelligens
description: AI hotar vår demokrati, vår teknologi och vår art.
---

AI är en kraftfull teknik som i allt större utsträckning förändrar vår värld.
Den kommer med fantastisk potential, men också med allvarliga risker, inklusive [existentiell katastrof](/xrisk).

## Nuvarande faror

### Falska nyheter, polarisering och hot mot demokratin

Mycket av vårt samhälle är baserat på tillit. Vi litar på att pengarna på vårt bankkonto är verkliga, att nyheterna vi läser är sanna, och att personerna som postar recensioner online existerar.

AI-system är exceptionellt bra på att skapa falska medier.
De kan skapa falska videor, falska ljud, falska texter och falska bilder.
Dessa kapaciteter förbättras snabbt.
För bara två år sedan skrattade vi åt de hemskt orealistiska Dall-E-bilderna, men nu har vi [deepfake-bilder som vinner fototävlingar](https://www.theguardian.com/technology/2023/apr/17/photographer-admits-prize-winning-image-was-ai-generated).
Ett 10-sekunders ljudklipp eller en enda bild kan vara nog för att skapa en övertygande deepfake.

Att skapa falska medier är inte nytt, men AI gör det mycket billigare och mycket mer realistiskt.
En AI-genererad bild av en explosion orsakade [panikförsäljning på Wall Street](https://www.euronews.com/next/2023/05/23/fake-news-about-an-explosion-at-the-pentagon-spreads-on-verified-accounts-on-twitter).
GPT-4 kan skriva på ett sätt som är omöjligt att skilja från människor, men i en mycket snabbare takt och till en bråkdel av kostnaden.
Vi kanske snart ser sociala medier översvämmas med falska diskussioner och åsikter, och falska nyhetsartiklar som är omöjliga att skilja från riktiga.

Detta leder till polarisering mellan olika grupper av människor som tror på olika informationskällor och narrativ och, genom att konsumera förvrängda representationer av vad som händer, eskalerar sina skillnader tills de kulminerar i våldsamma och antidemokratiska svar.

Ett stopp för frontlinjemodeller (vårt [förslag](/proposal)) skulle inte stoppa de modeller som används idag för att skapa falska medier, men det kan hjälpa till att förhindra framtida avancerade modeller.
Dessutom skulle det lägga grunden för framtida reglering som syftar till att mildra falska medier och andra specifika problem orsakade av AI. För att inte tala om att öka allmänhetens uppmärksamhet och medvetenhet om dessa faror och bevis på att de kan åtgärdas.

### Deepfakes och imitation

Falskt innehåll skapat med AI, även kallat deepfakes, kan inte bara stjäla kända personers identiteter och [skapa desinformation](https://time.com/6565446/biden-deepfake-audio/), utan de kan också imitera dig.
Alla med foton, videor eller ljud av någon och tillräcklig kunskap kan skapa deepfakes av dem och använda dem för att begå bedrägerier, trakassera dem eller skapa sexuellt icke-konsensuellt material.
Ungefär 96% av allt deepfake-innehåll är sexuellt material.

Som avsnittet om falska nyheter säger, skulle falska medier inte helt förhindras av vårt förslag, men de kunde minskas till viss del.
En inte så liten del när man tar i beaktande att AI-multifunktionssystem som chatbots har blivit väldigt populära, och vi skulle stoppa dem från att bli mer kapabla och populära, vilket kan inkludera system designade med färre filter och träningsbara med nya ansikten.

### Fördomar och diskriminering

AI-system tränas på data, och mycket av den data vi har är på något sätt partisk.
Detta innebär att AI-system kommer att ärva samhällets fördomar.
Ett automatiserat rekryteringssystem på Amazon [ärvde en fördom mot kvinnor](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).
Svarta patienter var [mindre benägna att bli hänvisade till en medicinsk specialist](https://www.science.org/doi/full/10.1126/science.aax2342).
Partiska system som används inom brottsbekämpning, såsom prediktiva polissystem, kan leda till orättvis måltavla mot specifika grupper.
Generativa AI-modeller kopierar inte bara fördomarna från deras träningsdata, [de förstärker dem](https://www.bloomberg.com/graphics/2023-generative-ai-bias/).
Dessa fördomar uppstår ofta utan att skaparna av AI-systemet är medvetna om dem.

<!-- ### Dataskydd
-->

### Jobbförlust, ekonomisk ojämlikhet och instabilitet

Under den industriella revolutionen förlorade många människor sina jobb till maskiner.
Men nya (ofta bättre) jobb skapades, och ekonomin växte.
Den här gången kan saker och ting vara annorlunda.

AI ersätter inte bara våra muskler som ångmaskinen gjorde, den ersätter våra hjärnor.
Vanliga människor kanske inte har något kvar att erbjuda ekonomin.
Bildgenereringsmodeller (som är kraftigt tränade på upphovsrättsskyddat material från professionella konstnärer) påverkar redan [den kreativa industrin](https://cointelegraph.com/news/artists-face-a-choice-with-ai-adapt-or-become-obsolete).
Skribenter [strejkar](https://www.newscientist.com/article/2373382-why-use-of-ai-is-a-major-sticking-point-in-the-ongoing-writers-strike/).
GPT-4 har [klarat advokatexamen](https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/), kan skriva utmärkt innehåll och kan skriva kod (återigen, delvis tränad på [upphovsrättsskyddat material](https://www.ischool.berkeley.edu/news/2023/new-research-prof-david-bamman-reveals-chatgpt-seems-be-trained-copyrighted-books)).

De som äger dessa AI-system kommer att kunna kapitalisera på dem, men de som förlorar sina jobb till dem kommer inte att göra det.
Det är svårt att förutsäga vilka jobb som kommer att ersättas först. De kan lämna dig arbetslös och utan inkomst oavsett hur mycket tid, pengar och energi du spenderat på att få den erfarenhet och kunskap som du har, och hur värdefulla de var för en stund sedan.
Det sätt vi fördelar rikedom i vårt samhälle är inte förberett för detta.

### Mental hälsa, beroende och avskildhet mellan människor

Sociala medier, videospel och annan programvara har redan använt AI-system för att maximera sin vinst medan de utnyttjar våra primathjärnor, vilket skadar vår mentala hälsa i processen.
Beroende av sociala medier, bland annat, isolerar oss från varandra, inte bara i politiska bubblor utan också i kulturella och sociala enmansbubblor, vilket gör oss ensammare.
De är det första beviset på de oavsiktliga och oväntade globala konsekvenser som dessa teknologier kan medföra och hur komplicerat det kan vara att anpassa AI-system med "mänskliga värderingar".

Om dagens chatbots fortsätter att bli bättre, kan det bli ganska vanligt att bli beroende av dem och ersätta hela relationer (vare sig romantiska, sexuella eller platoniska) med dem.
Även om dessa appar är lätta att komma åt, kan de forma förståelsen, personligheten och världsbilden hos barn som kan föredra att prata med AI istället för familj och vänner.
En paus i de största modellerna kan förhindra dem från att bli multifunktionella chatbots som passar våra behov perfekt utan att människor förstår de långsiktiga konsekvenserna av dem.

### Maktkoncentration, krig och kapplöpning mot avgrunden

Beroendet av produkter och tjänster som lär sig av personliga data lämnar oss som maktlösa separerade individer vare sig det är avsiktligt eller inte.
Och det spelar på en ond cirkel med koncentrationen av ekonomisk makt och intelligens hos de företag som skapar dem.

Om denna ekonomiska och teknologiska ojämlikhet härstammar från en handfull offentliga och privata enheter som producerar flera enkeländamåls-AI eller några multifunktions-AI, kan det leda till en kort maktkoncentration som sannolikt resulterar i en katastrof för alla.
Maktsamlingen av den berättelsen har och kommer att fortsätta att incitamentera fler aktörer

att gå med i kapplöpningen till botten och påskynda utvecklingen av större AI-system.
Detta introducerar fler felpunkter och förringar de tillhörande riskerna genom att stödja idén att de kan hanteras unilateralt, av ett företag eller en regering.

Ett sådant scenario skulle inte bara avmaktiga varje annan person och nation i världen, utan också katalysera globala makter att gå i konflikt.
Så det är avgörande att agera så snart som möjligt, innan tävlingsdynamiken sträcker sig längre, innan de redan mäktigaste regeringarna och företagen konsoliderar sina positioner och innan ett krig utlöses som svar.
Vi behöver internationellt samarbete eftersom det enda vinnande draget i detta konstiga spel är att inte spela, utan att pausa.

### Auktoritära regeringar
Auktoritära och totalitära regeringar kan också använda AI-teknologier för att utöva makt över sina territorier och befolkningar.
De kan kontrollera kommunikationskanaler eller upprätthålla sociala kredit- och massövervakningssystem som säkerställer att de behåller sin makt samtidigt som de kränker mänskliga rättigheter.

### Autonoma vapen
Autonoma vapen
Företag säljer redan AI-drivna vapen till regeringar.
Lanius bygger flygande självmordsdrönare som autonomt identifierar fiender.
Palantirs AIP-system använder stora språkmodeller för att analysera slagfältsdata och komma med optimala strategier.

Nationer och vapenföretag har insett att AI kommer att ha en enorm inverkan på att överträffa sina fiender.
Vi har gått in i en ny kapprustning.
Denna dynamik belönar att skynda på och skära hörn.

Just nu har vi fortfarande människor i loopen för dessa vapen.
Men när kapaciteten hos dessa AI-system förbättras kommer det att bli mer och mer tryck att ge maskinerna makten att fatta beslut.
När vi delegerar kontrollen över vapen till AI, kan fel och buggar få fruktansvärda konsekvenser.
Den hastighet med vilken AI kan bearbeta information och fatta beslut kan orsaka att konflikter eskalerar på några minuter.
En nyligen publicerad artikel konstaterar att "modeller tenderar att utveckla kapprustningsdynamik, vilket leder till större konflikter och i sällsynta fall till och med till användning av kärnvapen".

Läs mer på stopkillerrobots.org

## Nära framtida faror
### Biologiska vapen
AI kan göra kunskap mer tillgänglig, vilket också inkluderar kunskap om hur man skapar biologiska vapen. Denna artikel visar hur GPT-4 kan hjälpa icke-vetenskapliga studenter att skapa en pandemi-patogen:

På en timme föreslog chatbotarna fyra potentiella pandemi-patogener, förklarade hur de kan genereras från syntetiskt DNA med hjälp av omvänd genetik, försåg namnen på DNA-syntesföretag som troligen inte screenar beställningar, identifierade detaljerade protokoll och hur man felsöker dem, och rekommenderade att någon som saknar färdigheterna att utföra omvänd genetik engagerar en kärnanläggning eller ett kontraktsforskningsföretag.

Denna typ av kunskap har aldrig varit så tillgänglig, och vi har inte säkerhetsåtgärderna på plats för att hantera de potentiella konsekvenserna.

Dessutom kan vissa AI-modeller användas för att designa helt nya farliga patogener.
En modell kallad MegaSyn designade 40 000 nya kemiska vapen/toxiska molekyler på en timme.
Den revolutionära AlphaFold-modellen kan förutsäga strukturen av proteiner, vilket också är en dual-use technology.
Att förutsäga proteinstrukturer kan användas för att "upptäcka sjukdomsframkallande mutationer med hjälp av en individs genomsekvens".
Forskare skapar nu till och med helt autonoma kemiska laboratorier, där AI-system själva kan syntetisera nya kemikalier.

Den grundläggande faran är att kostnaden för att designa och använda biologiska vapen sänks med magnituder på grund av AI.

### Power accumulation and tyranny

Powerful AI models can be used to get more power.
This positive feedback loop can lead to a few companies or governments having an unhealthy amount of power.
Having control over thousands of intelligent, autonomous systems could be used to influence opinions, manipulate markets, or even wage war.
In the hands of an authoritarian government, this could be used to suppress dissent and maintain power.

<!--Dystopian social credit systems based on mass surveillance + people and action recognition could be created with today's AI so a pause wouldn't help and we shouldn't write about it-->

### Datorvirus och Cybersäkerhet 
Praktiskt taget allt vi gör idag är på något sätt beroende av datorer.
Vi betalar för våra dagligvaror, planerar våra dagar, kontaktar våra nära och kära och till och med kör våra bilar med datorer.

Moderna AI-system kan analysera och skriva programvara.
De kan hitta sårbarheter i programvara, och de kan användas för att utnyttja dem.
När AI-kapaciteten växer kommer också kapaciteten för de utnyttjanden de kan skapa att växa.

Mycket potenta datorvirus har alltid varit extremt svåra att skapa, men AI kan förändra detta.
Istället för att behöva anlita ett team av skickliga säkerhetsexperter/hackare för att hitta zero-day-sårbarheter, kan du bara använda en mycket billigare AI för att göra det åt dig. Naturligtvis kan AI också hjälpa till med cyberförsvar, och det är oklart på vilken sida fördelen ligger.

[Läs mer om AI och cybersäkerhetsrisker](/cybersecurity-risks)

Additionally, some AI models can be used to design completely new hazardous pathogens.
A model called MegaSyn designed [40,000 new chemical weapons / toxic molecules in one hour](https://www.theverge.com/2022/3/17/22983197/ai-new-possible-chemical-weapons-generative-models-vx).
The revolutionary AlphaFold model can predict the structure of proteins, which is also a [dual-use technology](https://unicri.it/sites/default/files/2021-12/21_dual_use.pdf).
Predicting protein structures can be used to "discover disease-causing mutations using one individual’s genome sequence".
Scientists are now even creating [fully autonomous chemical labs, where AI systems can synthesize new chemicals on their own](https://twitter.com/andrewwhite01/status/1670794000398184451).

The fundamental danger is that the cost of designing and applying biological weapons is being lowered by orders of magnitude because of AI.

### Computer viruses and cybersecurity attacks

Virtually everything we do nowadays is in some way dependent on computers.
We pay for our groceries, plan our days, contact our loved ones and even drive our cars with computers.

Modern AI systems can analyze and write software.
They [can find vulnerabilities](https://betterprogramming.pub/i-used-gpt-3-to-find-213-security-vulnerabilities-in-a-single-codebase-cc3870ba9411) in software, and [they could be used to exploit them](https://blog.checkpoint.com/2023/03/15/check-point-research-conducts-initial-security-analysis-of-chatgpt4-highlighting-potential-scenarios-for-accelerated-cybercrime/).
As AI capabilities grow, so will the capabilities of the exploits they can create.

Highly potent computer viruses have always been extremely hard to create, but AI could change that.
Instead of having to hire a team of skilled security experts/hackers to find zero-day exploits, you could just use a far cheaper AI to do it for you. Of course, AI could also help with cyberdefense, and it is unclear on which side the advantage lies.

[Read more about AI and cybersecurity risks](/cybersecurity-risks)
### Existentiell risk
Existentiell risk
Många AI-forskare varnar för att AI kan leda till mänsklighetens slut.

Mycket intelligenta saker är mycket kraftfulla. Om vi bygger en maskin som är mycket mer intelligent än människor, måste vi vara säkra på att den vill samma sak som vi vill. Detta visar sig dock vara mycket svårt. Detta kallas anpassningsproblemet. Om vi misslyckas med att lösa det i tid, kan vi sluta med superintelligenta maskiner som inte bryr sig om vårt välbefinnande. Vi skulle introducera en ny art på planeten som kan överlista oss och överträffa oss.

[Läs mer om x-risk](/xrisk)

### Mänsklig maktlöshet
Även om vi lyckas skapa endast AI-system som vi kan kontrollera individuellt, kan vi förlora vår makt att fatta viktiga beslut stegvis varje gång ett system implementeras inom institutioner eller blir populärt i vardagen. Dessa system skulle ha mer input från andra system än från människor, och om vi inte kan samordna oss tillräckligt snabbt, eller saknar nödvändig kunskap om hur dessa system fungerar, kan vi sluta utan kontroll över vår framtid.

Det skulle vara en civilisation där varje system optimerar för olika mål, det finns ingen klar riktning för vart allt är på väg, och det finns inget sätt att ändra det. Den tekniska kunskapen som krävs för att modifiera dessa system kan saknas från början eller gå förlorad över tid, när vi blir mer och mer beroende av teknik, och tekniken blir mer komplex.

Systemen kan uppnå sina mål, men dessa mål kanske inte helt inkapslar de värderingar de förväntades ha. Detta problem händer redan idag till viss del, men AI kan avsevärt förstärka det.

Även om vi lyckas skapa endast AI-system som vi kan kontrollera individuellt, skulle vi kunna förlora vår förmåga att fatta viktiga beslut stegvis varje gång ett system integreras i institutioner eller vardagen.
Dessa processer skulle i slutändan få mer input från AI-system än från människor, och om vi inte kan samordna oss tillräckligt snabbt, eller om vi saknar avgörande kunskap om systemens funktion, skulle vi kunna hamna utan kontroll över vår framtid.

### Digital medvetenhet
När AI fortsätter att utvecklas kan framtida system bli otroligt sofistikerade och replikera neurala strukturer och funktioner som liknar den mänskliga hjärnan. Denna ökade komplexitet kan leda till framväxande egenskaper som subjektivitet och/eller medvetande, så att dessa AI skulle förtjäna moraliska överväganden och behandlas väl. De skulle vara som "digitala människor". Problemet är att, med tanke på vår nuvarande brist på kunskap om medvetande och neurala nätverks natur, kommer vi inte att kunna avgöra om vissa AI skulle ha någon typ av upplevelse och vad kvaliteten på dessa upplevelser skulle bero på. Om AI fortsätter att produceras endast med deras kapacitet i åtanke, genom en process vi inte helt förstår, kommer människor fortsätta att använda dem som verktyg utan att förstå vad deras önskningar kan vara, och de kan faktiskt förslava digitala människor.

Risker med inlåst lidande
Det är möjligt att när automatisering på högre nivåer börjar ske, oavsett om det finns en eller flera kraftfulla AI, skulle värderingarna hos dessa system inte kunna ändras, och automatiseringen skulle fortsätta till universums slut, genom alla nåbara galaxer. De värsta scenarierna som dessa AI kan skapa skulle inte vara mänsklig utrotning, utan oundvikliga dystopier som skulle sträcka sig genom hela den tidsrymden.

Möjliga inlåsta dystopier med mycket lidande kallas S-risker och inkluderar världar där kännande varelser är förslavade och tvingas göra hemska saker. Dessa varelser kan vara människor, djur, digitala människor eller någon annan utomjordisk art som systemet kan hitta i kosmos. Med tanke på hur svårt vi tror att det är att helt lösa anpassningsproblemet, hur dåligt vi människor ibland behandlar varandra, hur illa vi behandlar de flesta djur, och hur vi behandlar nuvarande AI, kanske en framtid som denna inte är så osannolik som vi skulle vilja.

# Vad kan vi göra?
För alla problem som diskuterats ovan ökar risken när AI-förmågor förbättras. Detta innebär att det säkraste att göra nu är att sakta ner. Vi måste pausa utvecklingen av mer kraftfulla AI-system tills vi har listat ut hur vi ska hantera riskerna.

As AI continues to advance, future systems may become incredibly sophisticated, replicating neural structures and functions that are more akin to the human brain.
This increased complexity might lead to emergent properties like AI subjectivity and/or consciousness, which would make them deserving of moral considerations.

The problem is that, given our present lack of knowledge about consciousness and the nature of neural networks, we won't have a way to determine whether some AIs would have any type of experience and what the quality of those experiences would depend on.
If the AIs continue to be produced with only their capabilities in mind, through a process we don't fully understand, people will keep using them as tools ignoring what their desires could be, and that they could be actually enslaving "digital people".

### S-risker

Det är inte bara så att värdeinlåsning kan göra att vi misslyckas med att uppnå den bästa sortens världar, utan det kan också leda till att vi hamnar i dystopier värre än utrotning som kan sträcka sig över all rumtid.

Möjliga inlåsta dystopier med mycket lidande kallas _S-risker_(från engelskans suffering risk) och inkluderar världar där kännande varelser är förslavade och tvingade att göra hemska saker.
Dessa varelser kan vara människor, djur, digitala människor eller någon annan främmande art som AI:n kan hitta i kosmos. Med tanke på hur svårt vi tycker att det är att lösa allians helt och hållet, hur illa vi människor behandlar varandra ibland, hur illa vi behandlar de flesta djur och hur vi behandlar nuvarande AI:er, verkar en framtid som denna inte så osannolik som vi hoppas.

## Vad kan vi göra?

För **alla** problemen som diskuterats ovan ökar risken i takt med att AI:s kapacitet förbättras.
Det betyder att det säkraste att göra nu är att **sakta ner**. Vi måste pausa utvecklingen av kraftfullare AI-system tills vi har listat ut hur vi ska hantera riskerna.

Se vårt [förslag](/proposal) för mer detaljer.
