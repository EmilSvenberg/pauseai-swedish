---
title: FAQ
description: Vanliga frågor om PauseAI och riskerna med superintelligent AI.
---

<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

## Innehåll

## Vem är ni?

Vi är en gemenskap av [volontärer](/people) och [lokala gemenskaper](/communities) koordinerade av en [ideell organisation](/legal) som syftar till att mildra [riskerna med AI](/risks) (inklusive [risken för mänsklig utrotning](/xrisk)).
Vi strävar efter att övertyga våra regeringar att ingripa och [pausa utvecklingen av övermänsklig AI](/proposal).
Vi gör detta genom att informera allmänheten, prata med beslutsfattare och organisera protester.

Du kan hitta oss på [Discord](https://discord.gg/2XXWXvErfA) (det är här de flesta av våra samordningar sker!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI) och [Instagram](https://www.instagram.com/pause_ai).
Du kan maila/kontakta oss på [joep@pauseai.info](mailto:joep@pauseai.info).

## Är ni bara rädda för förändringar och ny teknik?

Du kanske blir förvånad över att de flesta i PauseAI betraktar sig själva som teknologiska optimister.
Många av dem är involverade i AI-utveckling, är gadgetälskare och har mestadels varit mycket entusiastiska över framtiden.
Särskilt många av dem har varit entusiastiska över AI:s potential att hjälpa mänskligheten.
Det är därför som den sorgliga insikten för många av dem att AI kan utgöra en existentiell risk var mycket [svår att internalisera](/psychology-of-x-risk).

## Vill ni förbjuda all AI?

Nej, bara utvecklingen av de största generella AI-systemen, ofta kallade "Frontier-modeller".
Nästan all befintlig AI skulle vara laglig enligt [vårt förslag](/proposal), och de flesta framtida AI-modeller kommer också att vara lagliga.
Vi kräver ett förbud mot AI-system som är kraftfullare än GPT-4, tills vi vet hur man bygger bevisligen säker AI, och vi har dem under demokratisk kontroll.

## Tror ni att GPT-4 kommer att döda oss?

Nej, vi tror inte att [nuvarande AI-modeller](/sota) är en existentiell risk.
Det verkar troligt att de flesta nästa AI-modeller inte heller kommer att vara det.
Men om vi fortsätter att bygga allt mer kraftfulla AI-system, kommer vi till slut att nå en punkt där en av dem blir en [existentiell risk](/xrisk).

## Kan en paus slå tillbaka och göra saker värre?

Vi har adresserat dessa farhågor i [denna artikel](/mitigating-pause-failures).

## Är en paus ens möjlig?

AGI är inte oundvikligt.
Det kräver horder av ingenjörer med miljonlöner.
Det kräver en fullt fungerande och obegränsad försörjningskedja av den mest komplexa hårdvaran.
Det kräver att vi alla tillåter dessa företag att spela med vår framtid.

[Läs mer om möjligheten av en paus](/feasibility).

## Vem betalar er?

Praktiskt taget alla våra åtgärder hittills har gjorts av volontärer.
Men sedan februari 2024 är PauseAI en [registrerad ideell stiftelse](/legal), och vi har fått flera donationer från enskilda individer.
Vi har också fått 20k finansiering från LightSpeed-nätverket.

Du kan också [donera](/donate) till PauseAI om du stöder vår sak!
Vi använder det mesta av pengarna för att möjliggöra för lokala gemenskaper att organisera evenemang.

## Vad är era planer?

Fokusera på [att växa rörelsen](/growth-strategy), organisera protester, lobbyverksamhet hos politiker och informera allmänheten.

Kolla in vår [roadmap](/roadmap) för en detaljerad översikt över våra planer och vad vi kan göra med mer finansiering.

## Hur tror ni att ni kan övertyga regeringar att pausa AI?

Kolla in vår [förändringsteori](/theory-of-change) för en detaljerad översikt av vår strategi.

## Varför protesterar ni?

- Att protestera visar världen att vi bryr oss om denna fråga. Genom att protestera visar vi att vi är villiga att lägga vår tid och energi för att få människor att lyssna.
- Protester kan och kommer ofta att [positivt påverka](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) allmän opinion, röstbeteende, företagsbeteende och policy.
- Överlägset [de flesta människor stödjer](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) fredliga/ickevåldsamma protester.
- Det finns [ingen "bakslag" effekt](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [såvida inte protesten är våldsam](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Våra protester är fredliga och ickevåldsamma.
- Det är en social bindningsupplevelse. Du träffar andra människor som delar dina bekymmer och vilja att agera.
- Kolla in [denna fantastiska artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) för mer insikter om varför protester fungerar.

Om du vill [organisera en protest](/organizing-a-protest), kan vi hjälpa dig med råd och resurser.

## Hur sannolikt är det att superintelligent AI kommer att orsaka mycket dåliga utfall, som mänsklig utrotning?

Vi har sammanställt [en lista med 'p(doom)'-värden](/pdoom) (sannolikhet för dåliga utfall) från olika framstående experter inom området.

AI-säkerhetsforskare (som är experter på detta ämne) är delade i denna fråga, och uppskattningar [sträcker sig från 2% till 97% med ett genomsnitt på 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Observera att inga (undersökta) AI-säkerhetsforskare tror att det finns en 0% chans.
Det kan dock finnas ett urvalsbias här: människor som arbetar inom AI-säkerhet gör det troligtvis för att de anser att det är viktigt att förhindra dåliga AI-utfall.

Om du frågar AI-forskare i allmänhet (inte säkerhetsspecialister), sjunker denna siffra till ett [medelvärde på runt 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), med en median på 5%.
En minoritet, cirka 20% av dem, tror att anpassningsproblemet inte är ett verkligt eller viktigt problem.
Observera att det kan finnas ett urvalsbias här i motsatt riktning: människor som arbetar inom AI gör det troligtvis för att de tror att AI kommer att vara fördelaktigt.

_Föreställ dig att du blir inbjuden att ta en provflygning på ett nytt flygplan_.
Flygplansingenjörerna tror att det finns en 14% chans att krascha.
Skulle du gå ombord på det planet? För just nu, går vi alla ombord på AI-planet.

## Hur lång tid har vi tills superintelligent AI?

Det kan ta månader, det kan ta decennier, ingen vet säkert.
Men vi vet att takten i AI-framsteg ofta underskattas grovt.
För bara tre år sedan trodde vi att vi skulle ha SAT-godkända AI-system år 2055.
Vi nådde dit i april 2023.
Vi bör agera som om vi har väldigt lite tid kvar eftersom vi inte vill bli tagna på sängen.

[Läs mer om brådskan](/urgency).

## Om vi pausar, vad händer med Kina?

Till att börja med har Kina vid denna tidpunkt striktare AI-regler än praktiskt taget något annat land.
De [tillät inte ens chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) och [förbjöd träning på internetdata](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) fram till [september 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
Kina har en mer kontrollerande regering och har därför ännu mer anledning att frukta de okontrollerbara och oförutsägbara effekterna av AI.
Under FN:s säkerhetsråds möte om AI-säkerhet var Kina det enda landet som nämnde möjligheten att genomföra en paus.

Observera också att vi främst begär en _internationell_ paus, verkställd genom ett fördrag.
Ett sådant fördrag behöver också undertecknas av Kina.
Om fördraget garanterar att andra nationer också kommer att stoppa, och det finns tillräckliga mekanismer för efterlevnad,
borde detta vara något som Kina också vill se.

## OpenAI och Google säger att de vill bli reglerade. Varför protesterar ni mot dem?

Vi applåderar [OpenAI](https://openai.com/blog/governance-of-superintelligence) och [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) för deras krav på internationell reglering av AI.
Men vi anser att de nuvarande förslagen inte är tillräckliga för att förhindra en AI-katastrof.
Google och Microsoft har ännu inte offentligt uttalat sig om den existentiella risken med AI.
Endast OpenAI [nämner uttryckligen risken för utrotning](https://openai.com/blog/governance-of-superintelligence), och vi applåderar dem återigen för att ta denna risk på allvar.
Men deras strategi är ganska tydlig: en paus är omöjlig, vi måste nå superintelligens först.
Problemet med detta är dock att de [inte tror att de har löst anpassningsproblemet](https://youtu.be/L_Guz73e6fw?t=1478).
AI-företagen är låsta i en kapplöpning mot botten, där AI-säkerhet offras för konkurrensfördelar.
Detta är helt enkelt resultatet av marknadsdynamik.
Vi behöver regeringar som ingriper och implementerar policyer (på internationell nivå) som [förhindrar de värsta utfallen](/proposal).

## Pushing AI-företagen den existentiella riskberättelsen för att manipulera oss?

Vi kan inte veta säkert vilka motiv dessa företag har, men vi vet att **existentiell risk inte initialt drevs av AI-företag - det var forskare, aktivister och NGO:er**.
Låt oss titta på tidslinjen.

Det har funnits många människor som har varnat för existentiell risk sedan början av 2000-talet.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark och många andra.
De hade ingen AI-teknik att driva - de var helt enkelt bekymrade över mänsklighetens framtid.

AI-företagen nämnde aldrig existentiell risk förrän mycket nyligen.

Sam Altman är ett intressant undantag.
Han skrev om existentiell AI-risk [redan 2015, på sin privata blogg](https://blog.samaltman.com/machine-intelligence-part-1), innan han grundade OpenAI.
Under åren sedan gjorde han praktiskt taget inga explicita nämnder av existentiell risk igen.
Under senatsförhöret den 16 maj 2023, när han tillfrågades om sitt blogginlägg om existentiell risk, svarade han bara genom att prata om jobb och ekonomi.
Han drev inte den existentiella riskberättelsen här, han undvek den aktivt.

I maj 2023 förändrades allt:

- Den 1 maj slutar 'Gudfadern av AI' Geoffrey Hinton [sitt jobb på Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) för att varna för existentiell risk.
- Den 5 maj tillkännages den [första PauseAI-protesten](https://twitter.com/Radlib4/status/1654262421794717696), precis vid OpenAI:s dörr.
- Den 22 maj publicerade OpenAI [ett blogginlägg om styrningen av superintelligens](https://openai.com/blog/governance-of-superintelligence), och nämnde existentiell risk för första gången.
- Den 24 maj erkänner ex-Google CEO Eric Schmidt existentiell risk.
- Den 30 maj publicerades [Safe.ai-uttalandet (som erkänner existentiell risk)](https://www.safe.ai/statement-on-ai-risk). Denna gång inkluderades människor från OpenAI, Google och Microsoft.

Dessa företag har varit mycket långsamma att erkänna existentiell risk, med tanke på att många av deras anställda har varit medvetna om det i åratal.
Så enligt vår syn är AI-företagen inte de som driver den existentiella riskberättelsen, de har reagerat på andra som driver den, och har väntat med sitt svar tills det var absolut nödvändigt.

Affärsincitamenten pekar i motsatt riktning: företag vill snarare inte att folk ska oroa sig för riskerna med deras produkter.
Praktiskt taget alla företag förminskar riskerna för att attrahera kunder och investeringar, snarare än att överdriva dem.
Hur mycket strikt reglering och negativ uppmärksamhet bjuder företagen in genom att erkänna dessa faror?
Och skulle ett företag som OpenAI [dedikera 20% av sina beräkningsresurser](https://openai.com/blog/introducing-superalignment) till AI-säkerhet om de inte trodde på riskerna?

Här är vår tolkning: AI-företagen undertecknade uttalandet eftersom _de vet att existentiell risk är ett problem som måste tas på största allvar_.

En stor anledning till att många andra människor fortfarande inte vill tro att existentiell risk är en verklig oro?
För att erkänna att _vi faktiskt är i fara_ är en mycket, mycket skrämmande sak.

[Läs mer om psykologin bakom existentiell risk](/psychology-of-x-risk).

## Okej, jag vill hjälpa! Vad kan jag göra?

Det finns många [saker du kan göra](/action).
På egen hand kan du skriva ett [brev](/writing-a-letter), posta [flyers](/flyering), informera [andra](/action#you-can-do-this), delta i en [protest](/protests), eller [donera](/donate) pengar!
Men ännu viktigare: du kan [gå med i PauseAI](/join) och samordna med andra som tar åtgärder.
Om du vill bidra mer kan du bli volontär och gå med i ett av våra [team](/teams).






