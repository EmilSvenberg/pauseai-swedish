---
title: FAQ
description: Vanliga frågor om PauseAI och riskerna med superintelligent AI.
---

<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

## Innehåll

## Vem är ni?

Vi är en gemenskap av [volontärer](/people) och [lokala gemenskaper](/communities) koordinerade av en [ideell organisation](/legal) som syftar till att mildra [riskerna med AI](/risks) (inklusive [risken för mänsklig utrotning](/xrisk)).
Vi strävar efter att övertyga våra regeringar att ingripa och [pausa utvecklingen av övermänsklig AI](/proposal).
Vi gör detta genom att informera allmänheten, prata med beslutsfattare och organisera protester.

Du kan hitta oss på [Discord](https://discord.gg/2XXWXvErfA) (det är här de flesta av våra samordningar sker!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI) och [Instagram](https://www.instagram.com/pause_ai).
Du kan maila/kontakta oss på [joep@pauseai.info](mailto:joep@pauseai.info).

## Är ni bara rädda för förändringar och ny teknik?

Du kanske blir förvånad över att de flesta i PauseAI betraktar sig själva som teknologiska optimister.
Många av dem är involverade i AI-utveckling, är gadgetälskare och har mestadels varit mycket entusiastiska över framtiden.
Särskilt många av dem har varit entusiastiska över AI:s potential att hjälpa mänskligheten.
Det är därför som den sorgliga insikten för många av dem att AI kan utgöra en existentiell risk var mycket [svår att internalisera](/psychology-of-x-risk).

## Vill ni förbjuda all AI?

Nej, bara utvecklingen av de största generella AI-systemen, ofta kallade "Frontier-modeller".
Nästan all befintlig AI skulle vara laglig enligt [vårt förslag](/proposal), och de flesta framtida AI-modeller kommer också att vara lagliga.
Vi kräver ett förbud mot AI-system som är kraftfullare än GPT-4, tills vi vet hur man bygger bevisligen säker AI, och vi har dem under demokratisk kontroll.

## Tror ni att GPT-4 kommer att döda oss?

Nej, vi tror inte att [nuvarande AI-modeller](/sota) är en existentiell risk.
Det verkar troligt att de flesta nästa AI-modeller inte heller kommer att vara det.
Men om vi fortsätter att bygga allt mer kraftfulla AI-system, kommer vi till slut att nå en punkt där en av dem blir en [existentiell risk](/xrisk).

## Kan en paus slå tillbaka och göra saker värre?

Vi har adresserat dessa farhågor i [denna artikel](/mitigating-pause-failures).

## Är en paus ens möjlig?

AGI är inte oundvikligt.
Det kräver horder av ingenjörer med miljonlöner.
Det kräver en fullt fungerande och obegränsad försörjningskedja av den mest komplexa hårdvaran.
Det kräver att vi alla tillåter dessa företag att spela med vår framtid.

[Läs mer om möjligheten av en paus](/feasibility).

## Vem betalar er?

Praktiskt taget alla våra åtgärder hittills har gjorts av volontärer.
Men sedan februari 2024 är PauseAI en [registrerad ideell stiftelse](/legal), och vi har fått flera donationer från enskilda individer.
Vi har också fått 20k finansiering från LightSpeed-nätverket.

Du kan också [donera](/donate) till PauseAI om du stöder vår sak!
Vi använder det mesta av pengarna för att möjliggöra för lokala gemenskaper att organisera evenemang.

## Vad är era planer?

Fokusera på [att växa rörelsen](/growth-strategy), organisera protester, lobbyverksamhet hos politiker och informera allmänheten.

Kolla in vår [roadmap](/roadmap) för en detaljerad översikt över våra planer och vad vi kan göra med mer finansiering.

## Hur tror ni att ni kan övertyga regeringar att pausa AI?

Kolla in vår [förändringsteori](/theory-of-change) för en detaljerad översikt av vår strategi.

## Varför protesterar ni?

- Att protestera visar världen att vi bryr oss om denna fråga. Genom att protestera visar vi att vi är villiga att lägga vår tid och energi för att få människor att lyssna.
- Protester kan och kommer ofta att [positivt påverka](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) allmän opinion, röstbeteende, företagsbeteende och policy.
- Överlägset [de flesta människor stödjer](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) fredliga/ickevåldsamma protester.
- Det finns [ingen "bakslag" effekt](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [såvida inte protesten är våldsam](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). Våra protester är fredliga och ickevåldsamma.
- Det är en social bindningsupplevelse. Du träffar andra människor som delar dina bekymmer och vilja att agera.
- Kolla in [denna fantastiska artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) för mer insikter om varför protester fungerar.

Om du vill [organisera en protest](/organizing-a-protest), kan vi hjälpa dig med råd och resurser.

## Hur sannolikt är det att superintelligent AI kommer att orsaka mycket dåliga utfall, som mänsklig utrotning?

Vi har sammanställt [en lista med 'p(doom)'-värden](/pdoom) (sannolikhet för dåliga utfall) från olika framstående experter inom området.

AI-säkerhetsforskare (som är experter på detta ämne) är delade i denna fråga, och uppskattningar [sträcker sig från 2% till 97% med ett genomsnitt på 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Observera att inga (undersökta) AI-säkerhetsforskare tror att det finns en 0% chans.
Det kan dock finnas ett urvalsbias här: människor som arbetar inom AI-säkerhet gör det troligtvis för att de anser att det är viktigt att förhindra dåliga AI-utfall.

Om du frågar AI-forskare i allmänhet (inte säkerhetsspecialister), sjunker denna siffra till ett [medelvärde på runt 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), med en median på 5%.
En minoritet, cirka 20% av dem, tror att anpassningsproblemet inte är ett verkligt eller viktigt problem.
Observera att det kan finnas ett urvalsbias här i motsatt riktning: människor som arbetar inom AI gör det troligtvis för att de tror att AI kommer att vara fördelaktigt.

_Föreställ dig att du blir inbjuden att ta en provflygning på ett nytt flygplan_.
Flygplansingenjörerna tror att det finns en 14% chans att krascha.
Skulle du gå ombord på det planet? För just nu, går vi alla ombord på AI-planet.

## Hur lång tid har vi tills superintelligent AI?

Det kan ta månader, det kan ta decennier, ingen vet säkert.
Men vi vet att takten i AI-framsteg ofta underskattas grovt.
För bara tre år sedan trodde vi att vi skulle ha SAT-godkända AI-system år 2055.
Vi nådde dit i april 2023.
Vi bör agera som om vi har väldigt lite tid kvar eftersom vi inte vill bli tagna på sängen.

[Läs mer om brådskan](/urgency).

## Om vi pausar, vad händer med Kina?

För det första har Kina i detta skede strängare AI-regler än praktiskt taget alla andra länder.
De [tillät inte ens chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points
