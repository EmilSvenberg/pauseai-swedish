---
title: FAQ
description: Vanliga fr친gor om PauseAI och riskerna med superintelligent AI.
---

<script>
    import SimpleToc from '$lib/components/simple-toc/SimpleToc.svelte'
</script>
<style>
    h2 {
        font-size: 1.2rem;
    }
</style>

<SimpleToc />

## Vilka 칛r ni?

Vi 칛r en gemenskap av [volont칛rer](/people) och [lokala gemenskaper](/communities) koordinerade av en [ideell organisation](/legal) som syftar till att mildra [riskerna med AI](/risks) (inklusive [risken f칬r m칛nsklig utrotning](/xrisk)).
Vi str칛var efter att 칬vertyga v친ra regeringar att ingripa och [pausa utvecklingen av 칬verm칛nsklig AI](/proposal).
Vi g칬r detta genom att informera allm칛nheten, prata med beslutsfattare och organisera protester.

Du kan hitta oss p친 [Discord](https://discord.gg/2XXWXvErfA) (det 칛r h칛r de flesta av v친ra samordningar sker!), [Twitter](https://twitter.com/PauseAI), [Substack](https://substack.com/@pauseai), [Facebook](https://www.facebook.com/PauseAI), [TikTok](https://www.tiktok.com/@pauseai), [LinkedIn](https://www.linkedin.com/uas/login?session_redirect=/company/97035448/), [YouTube](https://www.youtube.com/@PauseAI) och [Instagram](https://www.instagram.com/pause_ai).
Du kan maila/kontakta oss p친 [joep@pauseai.info](mailto:joep@pauseai.info).

## 츿r ni bara r칛dda f칬r f칬r칛ndringar och ny teknik?

Du kanske blir f칬rv친nad 칬ver att de flesta i PauseAI betraktar sig sj칛lva som teknologiska optimister.
M친nga av dem 칛r involverade i AI-utveckling, 칛r gadget칛lskare och har mestadels varit mycket entusiastiska 칬ver framtiden.
S칛rskilt m친nga av dem har varit entusiastiska 칬ver AI:s potential att hj칛lpa m칛nskligheten.
Det 칛r d칛rf칬r som den sorgliga insikten f칬r m친nga av dem att AI kan utg칬ra en existentiell risk var mycket [sv친r att internalisera](/psychology-of-x-risk).

## Vill ni f칬rbjuda all AI?

Nej, bara utvecklingen av de st칬rsta generella AI-systemen, ofta kallade "Frontier-modeller".
N칛stan all befintlig AI skulle vara laglig enligt [v친rt f칬rslag](/proposal), och de flesta framtida AI-modeller kommer ocks친 att vara lagliga.
Vi kr칛ver ett f칬rbud mot AI-system som 칛r kraftfullare 칛n GPT-4, tills vi vet hur man bygger bevisligen s칛ker AI, och vi har dem under demokratisk kontroll.

## Tror ni att GPT-4 kommer att d칬da oss?

Nej, vi tror inte att [nuvarande AI-modeller](/sota) 칛r en existentiell risk.
Det verkar troligt att de flesta n칛sta AI-modeller inte heller kommer att vara det.
Men om vi forts칛tter att bygga allt mer kraftfulla AI-system, kommer vi till slut att n친 en punkt d칛r en av dem blir en [existentiell risk](/xrisk).

## Kan en paus sl친 tillbaka och g칬ra saker v칛rre?

Vi har adresserat dessa farh친gor i [denna artikel](/mitigating-pause-failures).

## 츿r en paus ens m칬jlig?

AGI 칛r inte oundvikligt.
Det kr칛ver horder av ingenj칬rer med miljonl칬ner.
Det kr칛ver en fullt fungerande och obegr칛nsad f칬rs칬rjningskedja av den mest komplexa h친rdvaran.
Det kr칛ver att vi alla till친ter dessa f칬retag att spela med v친r framtid.

[L칛s mer om m칬jligheten av en paus](/feasibility).

## Vem betalar er?

Praktiskt taget alla v친ra 친tg칛rder hittills har gjorts av volont칛rer.
Men sedan februari 2024 칛r PauseAI en [registrerad ideell stiftelse](/legal), och vi har f친tt flera donationer fr친n enskilda individer.
Vi har ocks친 f친tt 20k finansiering fr친n LightSpeed-n칛tverket.

Du kan ocks친 [donera](/donate) till PauseAI om du st칬der v친r sak!
Vi anv칛nder det mesta av pengarna f칬r att m칬jligg칬ra f칬r lokala gemenskaper att organisera evenemang.

## Vad 칛r era planer?

Fokusera p친 [att v칛xa r칬relsen](/growth-strategy), organisera protester, lobbyverksamhet hos politiker och informera allm칛nheten.

Kolla in v친r [roadmap](/roadmap) f칬r en detaljerad 칬versikt 칬ver v친ra planer och vad vi kan g칬ra med mer finansiering.

## Hur tror ni att ni kan 칬vertyga regeringar att pausa AI?

Kolla in v친r [f칬r칛ndringsteori](/theory-of-change) f칬r en detaljerad 칬versikt av v친r strategi.

## Varf칬r protesterar ni?

- Att protestera visar v칛rlden att vi bryr oss om denna fr친ga. Genom att protestera visar vi att vi 칛r villiga att l칛gga v친r tid och energi f칬r att f친 m칛nniskor att lyssna.
- Protester kan och kommer ofta att [positivt p친verka](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) allm칛n opinion, r칬stbeteende, f칬retagsbeteende och policy.
- 칐verl칛gset [de flesta m칛nniskor st칬djer](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) fredliga/ickev친ldsamma protester.
- Det finns [ingen "bakslag" effekt](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) [s친vida inte protesten 칛r v친ldsam](https://news.stanford.edu/2018/10/12/how-violent-protest-can-backfire/). V친ra protester 칛r fredliga och ickev친ldsamma.
- Det 칛r en social bindningsupplevelse. Du tr칛ffar andra m칛nniskor som delar dina bekymmer och vilja att agera.
- Kolla in [denna fantastiska artikel](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) f칬r mer insikter om varf칬r protester fungerar.

Om du vill [organisera en protest](/organizing-a-protest), kan vi hj칛lpa dig med r친d och resurser.

## Hur sannolikt 칛r det att superintelligent AI kommer att orsaka mycket d친liga utfall, som m칛nsklig utrotning?

Vi har sammanst칛llt [en lista med 'p(doom)'-v칛rden](/pdoom) (sannolikhet f칬r d친liga utfall) fr친n olika framst친ende experter inom omr친det.

AI-s칛kerhetsforskare (som 칛r experter p친 detta 칛mne) 칛r delade i denna fr친ga, och uppskattningar [str칛cker sig fr친n 2% till 97% med ett genomsnitt p친 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results).
Observera att inga (unders칬kta) AI-s칛kerhetsforskare tror att det finns en 0% chans.
Det kan dock finnas ett urvalsbias h칛r: m칛nniskor som arbetar inom AI-s칛kerhet g칬r det troligtvis f칬r att de anser att det 칛r viktigt att f칬rhindra d친liga AI-utfall.

Om du fr친gar AI-forskare i allm칛nhet (inte s칛kerhetsspecialister), sjunker denna siffra till ett [medelv칛rde p친 runt 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), med en median p친 5%.
En minoritet, cirka 20% av dem, tror att anpassningsproblemet inte 칛r ett verkligt eller viktigt problem.
Observera att det kan finnas ett urvalsbias h칛r i motsatt riktning: m칛nniskor som arbetar inom AI g칬r det troligtvis f칬r att de tror att AI kommer att vara f칬rdelaktigt.

_F칬rest칛ll dig att du blir inbjuden att ta en provflygning p친 ett nytt flygplan_.
Flygplansingenj칬rerna tror att det finns en 14% chans att krascha.
Skulle du g친 ombord p친 det planet? F칬r just nu, g친r vi alla ombord p친 AI-planet.

## Hur l친ng tid har vi tills superintelligent AI?

Det kan ta m친nader, det kan ta decennier, ingen vet s칛kert.
Men vi vet att takten i AI-framsteg ofta underskattas grovt.
F칬r bara tre 친r sedan trodde vi att vi skulle ha SAT-godk칛nda AI-system 친r 2055.
Vi n친dde dit i april 2023.
Vi b칬r agera som om vi har v칛ldigt lite tid kvar eftersom vi inte vill bli tagna p친 s칛ngen.

[L칛s mer om br친dskan](/urgency).

## Om vi pausar, vad h칛nder med Kina?

For starters, at this point, China has stricter AI regulations than virtually any other country.
They [didn't even allow chatbots](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) and [disallowed training on internet data](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) up [until September 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation).
China has a more controlling government and thus has even more reason to fear the uncontrollable and unpredictable impacts of AI.
During the UNSC meeting on AI safety, China was the only country that mentioned the possibility of implementing a pause.

Also note that we are primarily asking for an _international_ pause, enforced by a treaty.
Such a treaty also needs to be signed by China.
If the treaty guarantees that other nations will stop as well, and there are sufficient enforcement mechanisms in place,
this should be something that China will want to see as well.

## OpenAI and Google are saying they want to be regulated. Why are you protesting them?

We applaud [OpenAI](https://openai.com/blog/governance-of-superintelligence) and [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) for their calls for international regulation of AI.
However, we believe that the current proposals are not enough to prevent an AI catastrophe.
Google and Microsoft have not yet publicly stated anything about the existential risk of AI.
Only OpenAI [explicitly mentions the risk of extinction](https://openai.com/blog/governance-of-superintelligence), and again we applaud them for taking this risk seriously.
However, their strategy is quite explicit: a Pause is impossible, we need to get to superintelligence first.
The problem with this, however, is that they [do not believe they have solved the alignment problem](https://youtu.be/L_Guz73e6fw?t=1478).
The AI companies are locked in a race to the bottom, where AI safety is sacrificed for competitive advantage.
This is simply the result of market dynamics.
We need governments to step in and implement policies (at an international level) that [prevent the worst outcomes](/proposal).

## Are AI companies pushing the existential risk narrative to manipulate us?

We can't know for certain what motivations these companies have, but we do know that **x-risk was not initially pushed by AI companies - it was scientists, activists and NGOs**.
Let's look at the timeline.

There have been many people who have warned about x-risk since the early 2000s.
Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark, and many others.
They had no AI tech to push - they were simply concerned about the future of humanity.

The AI companies never mentioned x-risk until very recently.

Sam Altman is an interesting exception.
He wrote about existential AI risk [back in 2015, on his private blog](https://blog.samaltman.com/machine-intelligence-part-1), before founding OpenAI.
In the years since he made virtually no explicit mention of x-risk again.
During the Senate hearing on May 16, 2023, when asked about his x-risk blog post, he only answered by talking about jobs and the economy.
He was not pushing the x-risk narrative here, he was actively avoiding it.

In May 2023, everything changed:

- On May 1st, 'Godfather of AI' Geoffrey Hinton [quits his job at Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) to warn about x-risk.
- On May 5th, the [first PauseAI protest is announced](https://twitter.com/Radlib4/status/1654262421794717696), right at OpenAI's doorstep.
- On May 22nd, OpenAI published [a blog post about the governance of superintelligence](https://openai.com/blog/governance-of-superintelligence), and mentioned x-risk for the first time.
- On May 24th, ex-Google CEO Eric Schmidt acknowledges x-risk.
- On May 30th, the [Safe.ai statement (acknowledging x-risk)](https://www.safe.ai/statement-on-ai-risk) was published. This time, including people from OpenAI, Google and Microsoft.

These companies have been very slow to acknowledge x-risk, considering that many of their employees have been aware of it for years.
So in our view, the AI companies are not pushing the x-risk narrative, they have been reactive to others pushing it, and have waited with their response until it was absolutely necessary.

The business incentives point in the other direction: companies would rather not have people worry about the risks of their products.
Virtually all companies downplay risks to attract customers and investments, rather than exaggerating them.
How much strict regulation and negative attention are the companies inviting due to admitting these dangers?
And would a company like OpenAI [dedicate 20% of its compute resources](https://openai.com/blog/introducing-superalignment) to AI safety if it wouldn't believe in the risks?

Here's our interpretation: the AI companies signed the statement because _they know that x-risk is a problem that needs to be taken very seriously_.

A big reason many other people still don't want to believe that x-risk is a real concern?
Because acknowledging that _we are in fact in danger_ is a very, very scary thing.

[Read more about the psychology of x-risk](/psychology-of-x-risk).

## Ok, I want to help! What can I do?

There are many [things that you can do](/action).
On your own, you can write a [letter](/writing-a-letter), post [flyers](/flyering), [learn](/learn) and inform others, join a [protest](/protests), ir [donating](/donate) some money!
But even more important: you can [join PauseAI](/join) and coordinate with others who are taking action.
Check out if there are [local communities](/communities) in your area.
If you want to contribute more, you can become a volunteer and join one of our [teams](/teams), or [set up a local community](/local-organizing)!

Even when facing the end of the world, there can still be hope and very rewarding work to do. 游눩
